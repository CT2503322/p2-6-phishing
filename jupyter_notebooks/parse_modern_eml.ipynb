{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enron Email Extraction\n",
    "\n",
    "This notebook extracts various fields from the Enron email dataset, including:\n",
    "- Message-ID\n",
    "- Date\n",
    "- From\n",
    "- To\n",
    "- Cc / Bcc\n",
    "- Subject\n",
    "- Mime-Version, Content-Type, Content-Transfer-Encoding\n",
    "- Body (plain text/HTML content)\n",
    "- Attachments# Enron Email Extraction\n",
    "\n",
    "This notebook extracts various fields from the Enron email dataset, including:\n",
    "- Message-ID\n",
    "- Date\n",
    "- From\n",
    "- To\n",
    "- Cc / Bcc\n",
    "- Subject\n",
    "- Mime-Version, Content-Type, Content-Transfer-Encoding\n",
    "- Body (plain text/HTML content)\n",
    "- Attachments\n",
    "\n",
    "## Additional Cleaning For Modern EML Files\n",
    "\n",
    "In addition, we had to clean the body, in the modern day, .eml files are usually cluttered with html, css, scripts and other artifacts. We took this a step further to make the parsing compatible with modern day emails. Keeping the user experience in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:14:01.343317Z",
     "iopub.status.busy": "2025-09-16T05:14:01.343026Z",
     "iopub.status.idle": "2025-09-16T05:14:02.249574Z",
     "shell.execute_reply": "2025-09-16T05:14:02.249126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported libraries successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import email\n",
    "import pandas as pd\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import warnings\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Imported libraries successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of known zero-width / formatting chars that often survive isprintable()\n",
    "ZERO_WIDTH = {\n",
    "    \"\\u034F\",       # Combining Grapheme Joiner (Mn) \"͏\"\n",
    "    \"\\u200B\", \"\\u200C\", \"\\u200D\",  # ZWSP/ZWNs/ZWJ (Cf)\n",
    "    \"\\uFEFF\",       # BOM / ZWNBSP (Cf)\n",
    "    \"\\u2060\",       # Word joiner (Cf)\n",
    "    \"\\u00AD\",       # Soft hyphen (Cf)\n",
    "}\n",
    "\n",
    "# Many wide/special spaces to normalize to a plain space\n",
    "SPACE_LIKE = {\n",
    "    \"\\u00A0\", \"\\u1680\", \"\\u2000\", \"\\u2001\", \"\\u2002\", \"\\u2003\",\n",
    "    \"\\u2004\", \"\\u2005\", \"\\u2006\", \"\\u2007\", \"\\u2008\", \"\\u2009\",\n",
    "    \"\\u200A\", \"\\u202F\", \"\\u205F\", \"\\u3000\"\n",
    "}\n",
    "\n",
    "def clean_tada_body(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # 1) Unicode normalize to simplify odd forms\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "    out = []\n",
    "    for ch in text:\n",
    "        # Keep newlines intact (we'll collapse multiples later)\n",
    "        if ch == \"\\n\":\n",
    "            out.append(\"\\n\")\n",
    "            continue\n",
    "\n",
    "        # Drop zero-width / formatting junk\n",
    "        if ch in ZERO_WIDTH or unicodedata.category(ch) == \"Cf\":\n",
    "            continue\n",
    "\n",
    "        # Normalize any exotic spaces to a regular space\n",
    "        if ch in SPACE_LIKE or ch.isspace():\n",
    "            out.append(\" \")\n",
    "        else:\n",
    "            out.append(ch)\n",
    "\n",
    "    text = \"\".join(out)\n",
    "\n",
    "    # 2) Collapse runs of spaces to a single space\n",
    "    text = re.sub(r\"[ ]{2,}\", \" \", text)\n",
    "\n",
    "    # 3) Clean spaces around newlines\n",
    "    text = re.sub(r\" *\\n *\", \"\\n\", text)\n",
    "\n",
    "    # 4) Limit multiple blank lines to max one blank line (e.g., two \\n)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    # 5) Trim each line and overall\n",
    "    text = \"\\n\".join(line.strip() for line in text.splitlines()).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:14:02.278329Z",
     "iopub.status.busy": "2025-09-16T05:14:02.278067Z",
     "iopub.status.idle": "2025-09-16T05:14:02.282871Z",
     "shell.execute_reply": "2025-09-16T05:14:02.282533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_html_with_bs4 function defined!\n"
     ]
    }
   ],
   "source": [
    "def clean_html_with_bs4(html_content):\n",
    "    \"\"\"\n",
    "    Clean HTML content using BeautifulSoup to extract readable text\n",
    "    while removing CSS, scripts, and other artifacts.\n",
    "    \n",
    "    Args:\n",
    "        html_content (str): Raw HTML content from email\n",
    "    \n",
    "    Returns:\n",
    "        str: Clean, readable text\n",
    "    \"\"\"\n",
    "    if not html_content:\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        \n",
    "        # Remove hidden elements (normally not visible)\n",
    "        for hidden in soup.find_all(style=lambda x: x and 'display:none' in x):\n",
    "            hidden.extract()\n",
    "        \n",
    "        # Extract text while trying to preserve some structure\n",
    "        clean_text = soup.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Clean up excessive whitespace and newlines\n",
    "        lines = [line.strip() for line in clean_text.split('\\n') if line.strip()]\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing HTML content: {e}\")\n",
    "        # Fallback: return original content if parsing fails\n",
    "        return html_content.strip()\n",
    "\n",
    "print(\"clean_html_with_bs4 function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:14:02.284630Z",
     "iopub.status.busy": "2025-09-16T05:14:02.284518Z",
     "iopub.status.idle": "2025-09-16T05:14:02.291721Z",
     "shell.execute_reply": "2025-09-16T05:14:02.291383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_modern_eml_file function defined!\n"
     ]
    }
   ],
   "source": [
    "# Function to parse individual EML files with modern HTML support\n",
    "def parse_modern_eml_file(file_path):\n",
    "    \"\"\"\n",
    "    Parse an EML file and extract all requested fields, with support for\n",
    "    HTML content cleaning using BeautifulSoup.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the EML file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Extracted email data\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        msg = BytesParser(policy=policy.default).parse(f)\n",
    "    \n",
    "    # Extract standard email headers\n",
    "    subject = msg.get('Subject', 'No Subject')\n",
    "    sender = msg.get('From', 'No Sender')\n",
    "    recipients = msg.get('To', 'No Recipients')\n",
    "    cc = msg.get('Cc', '')\n",
    "    bcc = msg.get('Bcc', '')\n",
    "    date = msg.get('Date', 'No Date')\n",
    "    message_id = msg.get('Message-ID', 'No Message-ID')\n",
    "    \n",
    "    # Extract MIME-related headers\n",
    "    mime_version = msg.get('Mime-Version', '')\n",
    "    content_type = msg.get('Content-Type', '')\n",
    "    content_transfer_encoding = msg.get('Content-Transfer-Encoding', '')\n",
    "    \n",
    "    # Decode subject if needed\n",
    "    try:\n",
    "        if subject:\n",
    "            decoded_subject = email.header.decode_header(subject)\n",
    "            subject = ''.join(part[0].decode(part[1] or 'utf-8') if isinstance(part[0], bytes) else str(part[0]) for part in decoded_subject)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Extract email body with HTML cleaning support\n",
    "    body = ''\n",
    "    attachments = []\n",
    "    \n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            content_type = part.get_content_type()\n",
    "            \n",
    "            # Handle plain text parts\n",
    "            if content_type == 'text/plain' and not part.get_filename():\n",
    "                part_body = part.get_payload(decode=True).decode('utf-8', errors='ignore')\n",
    "                if body:  # If we already have body, append\n",
    "                    body += '\\n' + part_body\n",
    "                else:\n",
    "                    body = part_body\n",
    "            \n",
    "            # Handle HTML parts with BeautifulSoup cleaning\n",
    "            elif content_type == 'text/html' and not part.get_filename():\n",
    "                html_content = part.get_payload(decode=True).decode('utf-8', errors='ignore')\n",
    "                clean_content = clean_html_with_bs4(html_content)\n",
    "                if body:  # If we already have body, append\n",
    "                    body += '\\n' + clean_content\n",
    "                else:\n",
    "                    body = clean_content\n",
    "            \n",
    "            # Handle attachments\n",
    "            elif part.get_filename():\n",
    "                attachments.append(part.get_filename())\n",
    "    else:\n",
    "        # Single part message\n",
    "        payload = msg.get_payload(decode=True)\n",
    "        if payload:\n",
    "            content_type = msg.get_content_type()\n",
    "            \n",
    "            if content_type == 'text/html':\n",
    "                body = clean_html_with_bs4(payload.decode('utf-8', errors='ignore'))\n",
    "                body = clean_tada_body(body)\n",
    "            else:\n",
    "                body = payload.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    return {\n",
    "        'file': file_path,\n",
    "        'message_id': message_id,\n",
    "        'date': date,\n",
    "        'from': sender,\n",
    "        'to': recipients,\n",
    "        'cc': cc,\n",
    "        'bcc': bcc,\n",
    "        'subject': subject,\n",
    "        'body': body,\n",
    "        'mime_version': mime_version,\n",
    "        'content_type': content_type,\n",
    "        'content_transfer_encoding': content_transfer_encoding,\n",
    "        'attachments': '; '.join(attachments) if attachments else ''\n",
    "    }\n",
    "\n",
    "print(\"parse_modern_eml_file function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:14:02.293492Z",
     "iopub.status.busy": "2025-09-16T05:14:02.293365Z",
     "iopub.status.idle": "2025-09-16T05:14:02.298248Z",
     "shell.execute_reply": "2025-09-16T05:14:02.297685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_modern_emails_from_folder function defined!\n"
     ]
    }
   ],
   "source": [
    "# Function to extract emails from folder with modern EML support\n",
    "def extract_modern_emails_from_folder(folder_path, limit=1000):\n",
    "    \"\"\"\n",
    "    Extract email data from all files in the folder with modern HTML support\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to folder containing EML files\n",
    "        limit (int): Maximum number of emails to process\n",
    "    \n",
    "    Returns:\n",
    "        list: List of extracted email data dictionaries\n",
    "    \"\"\"\n",
    "    emails_data = []\n",
    "    email_count = 0\n",
    "    \n",
    "    print(f\"Starting extraction from {folder_path}...\")\n",
    "    \n",
    "    if os.path.isfile(folder_path):\n",
    "        # If it's a single file, process it directly\n",
    "        try:\n",
    "            email_data = parse_modern_eml_file(folder_path)\n",
    "            emails_data.append(email_data)\n",
    "            email_count = 1\n",
    "            print(f'Successfully processed 1 email from {folder_path}')\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {folder_path}: {e}')\n",
    "    else:\n",
    "        # If it's a folder, walk through all files\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            if email_count >= limit:\n",
    "                break\n",
    "                \n",
    "            for file in files:\n",
    "                if email_count >= limit:\n",
    "                    break\n",
    "                    \n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    if file.endswith('.eml') or file.endswith('.msg'):\n",
    "                        email_data = parse_modern_eml_file(file_path)\n",
    "                        emails_data.append(email_data)\n",
    "                        email_count += 1\n",
    "                        \n",
    "                        if email_count % 100 == 0:\n",
    "                            print(f'Processed {email_count} emails...')\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f'Error processing {file_path}: {e}')\n",
    "                    \n",
    "            if email_count >= limit:\n",
    "                break\n",
    "    \n",
    "    print(f\"Extraction completed! Processed {len(emails_data)} emails.\")\n",
    "    return emails_data\n",
    "\n",
    "print(\"extract_modern_emails_from_folder function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:14:02.300368Z",
     "iopub.status.busy": "2025-09-16T05:14:02.300224Z",
     "iopub.status.idle": "2025-09-16T05:14:02.316651Z",
     "shell.execute_reply": "2025-09-16T05:14:02.316314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing TADA email with modern EML parser...\n",
      "\n",
      "=== TADA EMAIL PARSING RESULTS ===\n",
      "Subject: Ding Dong~ Your TADA Updates are Here! 🔔\n",
      "From: TADA <noreply@info.tada.global>\n",
      "Date: Mon, 08 Sep 2025 08:32:23 +0000\n",
      "Content-Type: text/html (Original)\n",
      "\n",
      "=== CLEANED BODY PREVIEW ===\n",
      "📞 New TADA Hotline Number\n",
      "From today onwards, TADA’s hotline has been updated.\n",
      "For any enquiries or support, please contact us at our new number:\n",
      "☎️ 6750 0833\n",
      "Kindly take note of our updated hotline for your convenience.\n",
      "Contact Us\n",
      "Heading to Downtown East? Get this!\n",
      "Need a break from the busy life?\n",
      "Until 31 October, use code [DTE2025] to save $5 OFF to/from Downtown East up to 2 redemptions!\n",
      "*Terms and conditions apply.\n",
      "Get the Deal!\n",
      "Exclusive Discounts in Thailand, Hong Kong, and Cambodia for ...\n",
      "\n",
      "Modern EML parser test completed!\n"
     ]
    }
   ],
   "source": [
    "# Test the parsing with the TADA email file\n",
    "tada_eml_path = 'tada.eml'\n",
    "\n",
    "if os.path.exists(tada_eml_path):\n",
    "    print('Parsing TADA email with modern EML parser...')\n",
    "    \n",
    "    # Parse the TADA email\n",
    "    tada_data = parse_modern_eml_file(tada_eml_path)\n",
    "    \n",
    "    # Convert to DataFrame for easier viewing\n",
    "    df_tada = pd.DataFrame([tada_data])\n",
    "    \n",
    "    print('\\n=== TADA EMAIL PARSING RESULTS ===')\n",
    "    print(f\"Subject: {tada_data['subject']}\")\n",
    "    print(f\"From: {tada_data['from']}\")\n",
    "    print(f\"Date: {tada_data['date']}\")\n",
    "    print(f\"Content-Type: {tada_data['content_type']} (Original)\")\n",
    "    print('\\n=== CLEANED BODY PREVIEW ===')\n",
    "    print(tada_data['body'][:500] + ('...' if len(tada_data['body']) > 500 else ''))\n",
    "    \n",
    "else:\n",
    "    print(f'TADA EML file {tada_eml_path} not found.')\n",
    "    tada_data = None\n",
    "\n",
    "print(\"\")\n",
    "print(\"Modern EML parser test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T05:14:02.318499Z",
     "iopub.status.busy": "2025-09-16T05:14:02.318370Z",
     "iopub.status.idle": "2025-09-16T05:14:02.321624Z",
     "shell.execute_reply": "2025-09-16T05:14:02.321305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed data saved to modern_emails_extracted.json\n",
      "Output file size: 1,884 bytes\n"
     ]
    }
   ],
   "source": [
    "# Save the extracted data to JSON if we have data\n",
    "if 'tada_data' in locals() and tada_data:\n",
    "    output_file = 'modern_emails_extracted.json'\n",
    "    \n",
    "    # Save as JSON with proper formatting\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({'emails': [tada_data], 'total_count': 1}, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f'Parsed data saved to {output_file}')\n",
    "    \n",
    "    # Show file details\n",
    "    file_size = os.path.getsize(output_file)\n",
    "    print(f'Output file size: {file_size:,} bytes')\n",
    "else:\n",
    "    print('No data to save - email parsing failed or file not found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"📞 New TADA Hotline Number\\nFrom today onwards, TADA’s hotline has been updated.\\nFor any enquiries or support, please contact us at our new number:\\n☎️ 6750 0833\\nKindly take note of our updated hotline for your convenience.\\nContact Us\\nHeading to Downtown East? Get this!\\nNeed a break from the busy life?\\nUntil 31 October, use code [DTE2025] to save $5 OFF to/from Downtown East up to 2 redemptions!\\n*Terms and conditions apply.\\nGet the Deal!\\nExclusive Discounts in Thailand, Hong Kong, and Cambodia for DBS Cardholders!\\nPlanning a trip to Thailand, Hong Kong, or Cambodia?\\nUntil 31 December 2025, you can save with TADA when paying with your\\nDBS Cards\\n!\\nThailand:\\nUse code\\n[DBSTADABK]\\nto get\\n40 THB OFF\\n.\\nHong Kong:\\nUse code\\n[DBSTADAHK]\\nto get\\n35 HKD OFF\\n.\\nCambodia:\\nUse code\\n[DBSTADAKH]\\nto get\\n4,000 riels OFF\\n.\\nRide TADA for easier and more rewarding journeys!\\nManage Payment Methods\\nHello, Hong Kong! Let’s Ride 🇭🇰\\nHeading to Hong Kong soon?\\nUntil 31 December 2025, you can enjoy 5x 20% OFF (capped at HK$25) when you book a TADA ride in the city with code [\\nHK25SG]\\n.\\nMore Info\\nTADA Mobility (Singapore) PTE. LTD.\\n40 Sin Ming Lane, Singapore 573958\\nYou can\\nunsubscribe\\nfrom our emails, if you need to.\\nIf you'd like to unsubscribe and stop receiving these emails\\nclick here\\n.\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tada_data['body']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
